from moviepy.editor import VideoFileClip
from IPython.display import HTML
import numpy as np
import cv2
import pickle
import matplotlib.pyplot as plt
import glob
from tracker_p import tracker_p
from past import past

## Bring in the transformation matrix and distorsion coefficient generated by "camera_cal.py"
dist_pickle =  pickle.load(open("./calibration_pickle.p", "rb"))
mtx = dist_pickle["mtx"]
dist = dist_pickle["dist"]

#provide a window mask for visualization purposes   
def window_mask(width, height, img_ref, center, level):
    output = np.zeros_like(img_ref)
    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width)):min(int(center+width),img_ref.shape[1])] = 1
    return output

def process_image(img):
    
# STEP 2 (PREPROCESSING)
    # Conventional image processing is done with the amazing function cv2.adavtiveThreshold (which needs an undistort gray scale image)
    # It subracts inensity (50 in this case) from an neighborhood area (25 pixels in this case)
    #img = clip1.get_frame(45) # if only one frame is needed
    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # if BGR
    img = cv2.undistort(img,mtx,dist,None,mtx)      
    imgBW = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    preprocessImage = cv2.adaptiveThreshold(imgBW, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, -50) #this function is awesome!  

    # Now comes the tricky part of mimic human behavior.. still on P3 mood (I am a human expert so I should better try... and fail)
    # As human I dont see time frame separately I think I see the present and some of the past sticks to my mind...
    # So lets make a class called past (past.py) and make an instance with the thoughts I need to remember...
    # last_chunk is a dynamic list with all past frames of preprocessImage being appeneded every time this past instance remeber is called 
    # ComplexImage is the combination oflast third, fifth and seventh frame
    last_bunch = remember.past_frames(preprocessImage, chunk_size = 10)
    some_frames_ago_1 = last_bunch[2]# what happened three frames ago? (remember the exclusive combination of 3rd and 12th frames ago)
    some_frames_ago_2 = last_bunch[4]# five frames? (exclusive combination of 5th and 14th)
    some_frames_ago_3 = last_bunch[6]# seven? (7th and 18th)
    ComplexImage = np.zeros_like(imgBW) # make an image that include all pixels that are 255
    ComplexImage[((some_frames_ago_1 == 255) | (some_frames_ago_2 == 255) | (some_frames_ago_3 == 255))] = 255 
    #cv2.imwrite('bWbn3.jpg', last_bunch[3]) # if snapshot needed
    #exit()

# STEP 3 (PERSPECTIVE TRANSFORM)
    # Defining prespective transformation area
    # this code was rewritten from Aaron Youtube video 
    img_size = (img.shape[1],img.shape[0])
    bot_width = 0.76 # percent of trapezpoid bottom height
    mid_width = 0.08 # percent of trapezoid middle height
    height_pct = 0.62 # 0.62 # pecent of trapezoid height
    bottom_trim = 0.935 # percent from top to bottom to avoid hood
    src = np.float32([[img.shape[1]*(0.5-mid_width/2), img.shape[0]*height_pct], [img.shape[1]*(0.5+mid_width/2), img.shape[0]*height_pct], [img.shape[1]*(0.5+bot_width/2), img.shape[0]*bottom_trim], [img.shape[1]*(0.5-bot_width/2), img.shape[0]*bottom_trim]])
    offset = img_size[0]*0.25
    dst = np.float32([[offset, 0], [img_size[0]-offset, 0], [img_size[0]-offset, img_size[1]], [offset, img_size[1]]])

    # perfom transform
    M = cv2.getPerspectiveTransform(src,dst) # get the perspective transform matrix to create the warp prespective
    Minv = cv2.getPerspectiveTransform(dst,src) # get the inverse matrix, we want to see go back later on
    warped = cv2.warpPerspective(ComplexImage,M,img_size,flags=cv2.INTER_LINEAR) # create the birds eye version   
    #ComplexImage = np.array(cv2.merge((ComplexImage,ComplexImage,ComplexImage)),np.uint8) # if video of ComplexImage is needed
    #result_1 = ComplexImage

# STEP 4 (LANE-LINE PIXEL IDENTIFICATION AND POLYNOMIAL FITTING)
    
    # the following code was rewritten from Aaron's youtube video   
    # window_centroid is the averaged x position if the last 40 sets of 5 centers of sliding windows, see tracked_p.py
    window_centroids = curve_centers.find_window_centroids(warped)
    
    # extract x position to two different lists and make the sliding windows visible
    l_points = np.zeros_like(warped) # prepare a 1280 x 720 zeros array with for left points   
    r_points = np.zeros_like(warped) # prepare a 1280 x 720 zeros array with for left points
    leftx = [] # prepare a list to allocate left lane x positions
    rightx = [] # preoare a list to allocate right lane x positions
    for level in range(0,len(window_centroids)): # extract every x postion for every of the 5 leves, one by one
        leftx.append(window_centroids[level][0]) # extract one x position for the left lane and add it to the left list
        rightx.append(window_centroids[level][1]) # do the same for the right lane x point
        l_mask = window_mask(window_width, window_height, warped, window_centroids[level][0],level) # get a black 1280 x 720 with one white left window
        r_mask = window_mask(window_width, window_height, warped, window_centroids[level][1],level) # another one for the right window
        l_points[(l_points == 255) | ((l_mask == 1))] = 255 # add the left white window to the black 1280 x 720 that is accumulating them
        r_points[(r_points == 255) | ((r_mask == 1))] = 255 # do the same for the right window         
    template = np.array(r_points+l_points,np.uint8) # mix both 1280x720 array into one that have both sets of windows
    zero_channel = np.zeros_like(template) # make another 1280x720 zero array
    template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # create and RGB with the white windows array in the green channel
    warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8) # create a white RGB image of  the warped road
    result_1 = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # add "_1" to return line bellow to see birds eye vision of the above RGBs

    # perform the line fitting with stracted x vales for each lane
    yvals = range(0,warped.shape[0]) # make a list with numbers 0 to 719 for the values of y axis coordinates
    res_yvals = np.arange(warped.shape[0]-(window_height/2),0,-window_height) # get an array of the 5 points along y axis
    left_fit = np.polyfit(res_yvals, leftx,2) # get a vector with the coefficients of a 2nd order polynomial that fit the line expressed in res_yvals against leftx
    left_fitx = left_fit[0]*yvals*yvals + left_fit[1]*yvals + left_fit[2] # get an array with all the values of x coordinate of line with the above coefficients
    left_fitx = np.array(left_fitx,np.int32) # just get the integers
    right_fit = np.polyfit(res_yvals, rightx,2) # get a vector with the coefficients of a 2nd order polynomial that fit the line expressed in res_yvals against rightx
    right_fitx = right_fit[0]*yvals*yvals + right_fit[1]*yvals + right_fit[2] # get an array with all the values of y coordinate of line with the above coefficients
    right_fitx = np.array(right_fitx,np.int32) # just get the integers

# STEP 6 (LANE AREA IDENTIFICATION ON UNWARPED IMAGE)

    middle_marker = np.array(list(zip(np.concatenate((left_fitx-window_width/1, right_fitx[::-1]+window_width/2), axis=0), np.concatenate((yvals,yvals[::-1]), axis=0))),np.int32)
    road = np.zeros_like(img)
    cv2.fillPoly(road,[middle_marker],color=[0, 255, 0])
    road_unwarped = cv2.warpPerspective(road,Minv,img_size,flags=cv2.INTER_LINEAR)
    result = cv2.addWeighted(img,1.0,road_unwarped,0.7,0.0)

# STEP 5 (CURVATURE AND VEHICLE POSITION CLACULATION)

    ym_per_pix = curve_centers.ym_per_pix
    xm_per_pix = curve_centers.xm_per_pix
    curve_fit_cr = np.polyfit(np.array(res_yvals,np.float32)*ym_per_pix, np.array(leftx,np.float32)*xm_per_pix,2)
    curverad = ((1 +(2*curve_fit_cr[0]*yvals[-1]*ym_per_pix + curve_fit_cr[1])**2)**1.5) /np.absolute(2*curve_fit_cr[0])
    camera_center = (left_fitx[-1] + right_fitx[-1])/2
    center_diff = (camera_center-warped.shape[1]/2)*xm_per_pix
    side_pos = 'left'
    if center_diff <= 0:
        side_pos = 'right'

    cv2.putText(result,'Radius of Curvature = ' +str(round(curverad,3))+'(m)',(50,50), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,255,255),2)
    cv2.putText(result,'Vehicle is '+str(abs(round(center_diff,3)))+'m '+side_pos+' of center' ,(50,100) , cv2.FONT_HERSHEY_SIMPLEX, 1,(255, 255,255),2)

#    write_name = './hereso_1.jpg'#'./test_images/tracked_1_'+str(idx)+'.jpg'
#    cv2.imwrite('raw_45.jpg', img1)
#    cv2.imwrite('raw_44_9.jpg', img2)
#    cv2.imwrite('raw_44_8.jpg', img3)
#    cv2.imwrite('raw_44_7.jpg', img4)
#    cv2.imwrite('bin_44_8.jpg',preprocessImage3) cv2.imwrite('bin_44_7.jpg',
#    preprocessImage4)
#    cv2.imwrite('result_45_44_9_8_7_what.jpg',result_1)
     #exit()
    return result

# initialize instance remember of class past, chunk_size is size of buffer
remember = past(chunk_size=10)

# initialize instance curve_centers of tracker_p, My_ym = 30 meters per 720pixels along y axis of transformed image, My_xm = 5 meters per 640 pixels along x axis of wraped image=
# window_width amd window_height are the dimentions of the sliding window that detects lan pixels
# curve_center returns the average value of the last 40 sets of centers (Mysmooth_factor)
# every set of centes is a pair (left_lane and right_lane)  per every level (image_height=720 over window_height=72, 10 leves in this particular case) in each frame
window_width = 5
window_height = 144
curve_centers = tracker_p(Mywindow_width = window_width, Mywindow_height = window_height, Mymargin = 45, My_ym = 30/720, My_xm = 5/640, Mysmooth_factor = 20 ) #10/720 4/384

Output_video = 'P4_video_n25c50_bh246_ww5wh144m45f20.mp4'
Input_video= 'project_video.mp4'
clip1 = VideoFileClip(Input_video)#.subclip(1,6)
video_clip = clip1.fl_image(process_image)
video_clip.write_videofile(Output_video,audio=False)

